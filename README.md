# ğŸ¤– BipedalWalker-v3 Reinforcement Learning Project

A Reinforcement Learning project using **Proximal Policy Optimization (PPO)** algorithm to train an agent to walk in the **BipedalWalker-v3** environment (from Gymnasium / Box2D physics engine).

This project demonstrates:
- Gymnasium + Box2D physics simulation
- PPO training & testing pipeline using Stable-Baselines3
- GPU acceleration (CUDA PyTorch)
- Reward logging and plotting
- Cross-platform setup (Windows/Linux)
- Clean and reproducible project structure

---

## âš™ï¸ Tech Stack

| Component | Version |
|------------|----------|
| **Python** | 3.10.19 |
| **Gymnasium** | Latest |
| **Stable-Baselines3** | Latest |
| **PyTorch** | CUDA 12.1 (for GPU) / CPU fallback |
| **OS** | Windows 10/11 / Zorin OS 18 (Linux) |
| **GPU** | NVIDIA RTX 3050 Laptop GPU |

---

## ğŸ“‚ Project Structure

BipedalWalker-RL/  
â”‚  
â”œâ”€â”€ train.py                # Main training script  
â”œâ”€â”€ test.py                 # Testing and rendering trained model  
â”œâ”€â”€ plot_training.py        # Plot training rewards  
â”œâ”€â”€ check_gpu.py            # Check CUDA/GPU availability  
â”‚  
â”œâ”€â”€ requirements.txt        # Dependencies list  
â”œâ”€â”€ training_rewards.npy    # Stored reward data (auto-generated)  
â”œâ”€â”€ models/                 # Saved trained models  
â”œâ”€â”€ results/                # Optional log/plot directory  
â”œâ”€â”€ .gitignore              # Git ignore file  
â””â”€â”€ README.md               # Project documentation  

---

## ğŸš€ Installation & Setup

### 1ï¸âƒ£ Clone the Repository
https://github.com/SaumyaShah0/BipedalWalker-RL/


### 2ï¸âƒ£ Create Virtual Environment

**Windows:**
python -m venv walker_env
walker_env\Scripts\activate


### 3ï¸âƒ£ Install Dependencies
pip install --upgrade pip
pip install gymnasium[box2d] stable-baselines3 torch matplotlib pygame


---

## ğŸ§  Training the Model

Run:
python train.py


This will:
- Create the BipedalWalker-v3 environment
- Train PPO for 500,000 timesteps
- Save rewards to training_rewards.npy
- Export trained model as ppo_bipedalwalker.zip in /models/

---

## ğŸ® Testing the Trained Model

Run:
python test.py

This:
- Loads the trained model
- Opens the simulation window (2D graphical)
- Lets the agent walk in the terrain

---

## ğŸ§© PPO Algorithm Overview

**Full Form:** Proximal Policy Optimization  
PPO is a policy gradient method that balances exploration and stability by limiting updates to the policy function.  
It is widely used for continuous control problems such as robotics.

**ğŸ” Why PPO?**
- Stable updates (clip function prevents large policy changes)
- Works with both discrete & continuous actions
- Efficient for high-dimensional environments (like BipedalWalker)
- Easier to tune than TRPO or A2C

**âš”ï¸ Comparison**

| Algorithm | Full Form                         | Key Feature                  | Suitable For            | Comparison                                   |
|-----------|-----------------------------------|------------------------------|------------------------|-----------------------------------------------|
| PPO       | Proximal Policy Optimization      | Stable policy updates        | Continuous/Discrete    | âœ… Best balance of stability & performance    |
| A2C       | Advantage Actor-Critic            | Synchronous actor-critic     | Simpler problems       | âŒ Less stable for long training              |
| DDPG      | Deep Deterministic Policy Gradient| Continuous deterministic control| Robotics arms      | âš ï¸ Unstable w/o tuning                       |
| SAC       | Soft Actor-Critic                 | Entropy-regularized exploration| Continuous tasks    | âš¡ Fast but heavier compute cost              |

---

## ğŸ§± Hard Mode

To switch to hard terrain (with pits, stumps, and gaps):

In train.py and test.py:
env = gym.make("BipedalWalkerHardcore-v3")


---

## âš™ï¸ Performance

| Mode     | Hardware         | Training Time (500k Steps) |
|----------|------------------|---------------------------|
| Normal   | CPU              | ~50â€“60 minutes            |
| Normal   | GPU (RTX 3050)   | ~20â€“30 minutes            |
| Hardcore | GPU              | 2â€“4 hours                 |

---

## ğŸ§® GPU / CPU Usage

You can check CUDA or GPU availability using check_gpu.py:
import torch

print("Torch version:", torch.version)
print("CUDA available:", torch.cuda.is_available())

if torch.cuda.is_available():
print("GPU Name:", torch.cuda.get_device_name(0))
else:
print("Running on CPU only")

**ğŸ”§ Run on GPU**

If your system has CUDA and PyTorch with GPU support installed:

- It will automatically use GPU.
- No code changes needed.

If not detected:
pip uninstall torch
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121


---

## ğŸ“Š Plot Training Rewards

Run:
python plot_training.py

Example code:
import numpy as np
import matplotlib.pyplot as plt

rewards = np.load("training_rewards.npy")
plt.plot(rewards)
plt.xlabel("Episodes")
plt.ylabel("Reward")
plt.title("PPO on BipedalWalker-v3")
plt.grid(True)
plt.show()


## ğŸ“š References

- Gymnasium Documentation
- Stable-Baselines3 Docs
- PyTorch CUDA Install Guide
- PyLessons PPO Tutorial

## ğŸ Summary

This project trains a Bipedal robot to walk using PPO (Proximal Policy Optimization) with GPU acceleration.  
It demonstrates continuous control learning, policy gradient algorithms, and environment simulation â€” a foundational RL project for robotics and AI.
